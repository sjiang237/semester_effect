---
title: "Machine_Learning_Days"
author: "Shuheng Jiang"
date: "2024-10-19"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Setup

### Handle conflicts
```{r}
#| message: false
options(conflicts.policy = "depends.ok")
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/fun_ml.R?raw=true")
tidymodels_conflictRules()
# We also will need to resolve a new conflict using the following code
# Alternatively, John demonstrates code you can use when you load this library to prevent conflicts
conflictRules("Matrix", mask.ok = c("expand", "pack", "unpack"))
```

### Load required packages 
```{r}
#| message: false
#| warning: false
library(kableExtra, exclude = "group_rows") # for displaying formatted tables w/ kbl()
library(janitor, include.only = c("clean_names", "tabyl"))
library(cowplot, include.only = "plot_grid")
library(tidyverse) # for general data wrangling
library(tidymodels) # for modeling
library(kknn)
library(discrim, exclude = "smoothness")
library(xfun, include.only = "cache_rds")
library(cowplot, include.only = "plot_grid")
library(readxl)
library(dplyr)
library(lubridate)
library(skimr)
library(tidyselect)
library(tibble)
library(effectsize)
library(vip, exclude = "titanic")
detach("package:vip", unload = TRUE)
library(DALEX, exclude= "explain")
library(DALEXtra)
```

Be sure to initiate parallel processing and set up your path
```{r}
cl <- parallel::makePSOCKcluster(parallel::detectCores(logical = FALSE))
doParallel::registerDoParallel(cl)
```

### Source function scripts
```{r}
#| message: false
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/fun_plots.R?raw=true")
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/fun_eda.R?raw=true")
source("C:/Users/16089/OneDrive/Desktop/lab psych610/610710_functions.R")
```


### Specify other global settings
```{r}
#| include: false
theme_set(theme_classic())
options(tibble.width = Inf, dplyr.print_max=Inf)
rerun_setting <- TRUE
```

### file path
```{r}
file_path <- "C:/Users/16089/OneDrive/Desktop/Capstone_Project/semester_effect/cleaned_data_1"
```

```{r}
d_try <- read_csv(here::here(file_path, "Generic_Post_Survey_Combined.csv"), 
                   col_types = cols())|>
  glimpse()
```

```{r}
d_try$semester_phase <- cut(
  d_try$days_since_semester_start,
  breaks = c(-Inf, 39, 74, Inf),  # Define breakpoints based on days
  labels = c("start", "middle", "end"),
  include.lowest = TRUE
)
```
Linear SVM; show whether the group differ by mixed components, then 

```{r}
d_try <- d_try %>% select(participant_id, semester_phase, everything())
```

```{r}
d_try <- d_try %>% select(-days_since_semester_start)
```


## EDA for Data Cleaning
### Variable Classes and Tidy Names

Note that from our glimpse() above that all variable names are already lower case/snake case, but year should not be treated as double, adhd_clinical from the Numeric EDA below(or the background knowledge I have when I'm computing the composite score), should be treated as factor variable. 

```{r}
d_try <- d_try |>
  mutate(across(where(is.character), factor)) |>
  mutate(adhd_clinical = factor(adhd_clinical, levels = c("0", "1"), labels = c("no", "yes")),
         year = factor(year, levels = 2018:2023)) |>
  glimpse()
```

### Skim Data
```{r}
d_try|> 
  skim_some() 
```
**Variable class notes:**
*All variables are type factor, type numeric, or date and time. Some changes were made so that the classes as they were read in match their characteristics. The year variable represents the year participants answer the survey, so we class it as factor. We also classed adhd_clinical as factor with labels 'no' and 'yes'*


### Missing Data

```{r eval=FALSE}
d_try |> 
  skim_some() |> 
  select(skim_variable, n_missing, complete_rate)
```


```{r}
d_try |> filter(is.na(adhd_clinical)) |> 
  kbl(align = "r") |> 
  kable_styling(bootstrap_options = c("striped", "condensed")) |> 
  scroll_box(height = "500px", width = "100%")
```
*These might be the data you want to remove*


```{r}
d_try |> filter(is.na(growth_mindset_score)) |> 
  kbl(align = "r") |> 
  kable_styling(bootstrap_options = c("striped", "condensed")) |> 
  scroll_box(height = "500px", width = "100%")
```

*For those who have NAs for growth mindset also has NAs for matrices tests scores*

### Handle Missingness 
```{r}
ids_to_remove <- c("SEP_03_0020", "SEP_03_0039", "SEP_03_00240")  # Replace with your actual participant IDs

# Remove the specified participant IDs from the dataset
d_try <- d_try %>%
  filter(!participant_id %in% ids_to_remove)
```
*Need to further discuss how to handle the missing values for matrices tests and mindset tests* 

```{r}
d_try <- d_try |>
  filter(!is.na(ucmrt_hit_rate) & !is.na(matrices_hit_rate) & !is.na(sandia_matrices_hit_rate))
```

```{r eval=FALSE}
d_try |> 
  skim_some() |> 
  select(skim_variable, n_missing, complete_rate)
```

*Still Growth Mindset Score with missing data left*

```{r}
d_try |> filter(is.na(growth_mindset_score)) |> 
  kbl(align = "r") |> 
  kable_styling(bootstrap_options = c("striped", "condensed")) |> 
  scroll_box(height = "500px", width = "100%")
```

*For those who only miss the growth mindset score, we want to impute the score using median*

### Numeric variables
```{r eval=FALSE}
# skim data, looking at numeric min and max values
d_try |>
  skim_some() |> 
  filter(skim_type == "numeric") |>  # Select only numeric variables since min/max only apply to them
  select(skim_variable, numeric.p0, numeric.p100)
```
*Here, no responses seem out of the ordinary*

### Categorical Variables
**Tidy response labels for categorical variables:** 
```{r}
# tidy the response
d_try <- d_try |> 
  mutate(across(where(is.factor), tidy_responses))

d_try |> 
  select(semester_id, year, academic_year,semester, adhd_clinical, adhd_total) |>
  walk(\(column) print(levels(column)))
```

Now that we have completed data cleaning, we will split our data into train and test sets and save out the cleaned files.

### Generate a Train Test Split
We'll set a seed so that the same split can be redone if needed (like if we find more errors during modeling EDA/feature engineering & need to come back to this script). Assign 25% of the data to be our validation set. Stratify this split on the `days_since_semester_start` outcome variable.
```{r}
set.seed(0429)
splits <- d_try |> 
  initial_split(prop = 3/4, strata = "semester_phase", breaks = 4)
```

### Save cleaned files

```{r}
splits |> 
  analysis() |>
  glimpse() |> 
  write_csv(here::here(file_path, "data_2_train.csv"))

splits |> 
  assessment() |>
  glimpse() |> 
  write_csv(here::here(file_path, "data_2_test.csv"))
```
Complete modeling EDA on the cleaned training data so that I'm able to do good feature engineering & model fitting

## EDA for Modeling
```{r}
clean_class <- function(df){
  
  df |>
  mutate(across(where(is.character), factor)) |>
  mutate(adhd_clinical = fct_relevel(adhd_clinical, c("yes", "no")),
         year = factor(year, levels = 2018:2023),
         semester_phase = fct_relevel(semester_phase, c("start", "middle", "end"))) 
}
```

```{r}
data_trn <- 
  read_csv(here::here(file_path, "data_2_train.csv"), 
           col_types = cols()) |> 
  clean_class() |>
  glimpse()

data_test <- 
  read_csv(here::here(file_path, "data_2_test.csv"), 
           col_types = cols()) |> 
  clean_class()
```


```{r}
data_trn |> skim_all()
```

### Univaraite Exploration of Numeric variables
```{r}
data_trn |> 
  select(grit_overall, matrices_hit_rate, ucmrt_hit_rate, sandia_matrices_hit_rate, neuroticism_score, extraversion_score, conscientiousness_score, openness_score, agreeableness_score, adhd_total, growth_mindset_score, grit_effort, grit_interest, grit_overall) |>
  names() |> 
  map(\(name) plot_box_violin(df = d_try, x = name)) |> 
  plot_grid(plotlist = _, ncol = 4)
```

### Bivariate Exploration of Numeric Variables
#### Correaltion Matrix
```{r}
data_trn |>
  select(grit_overall, matrices_hit_rate, ucmrt_hit_rate, sandia_matrices_hit_rate, neuroticism_score, extraversion_score, conscientiousness_score, openness_score, agreeableness_score, growth_mindset_score, grit_effort, grit_interest, grit_overall, adhd_total) |> 
  cor(use = "pairwise.complete.obs") |> 
  corrplot::corrplot.mixed(tl.pos = "lt")
```


#### Table for Summary Statistics
```{r}
data_trn |> 
  skim_all() |> 
  filter(skim_type == "numeric")
```

### Univaraite Exploration of Categorical variables
This is a binomial distribution
```{r}
ggplot(data_trn, aes(x = semester_id)) +
  geom_histogram(stat = "count") +
  labs(title = "Histogram of Semester ID Counts",
       x = "Semester ID",
       y = "Count")
```

### Univaraite Exploration of Categorical variables
This is a binomial distribution
```{r}
ggplot(data_trn, aes(x = semester_phase)) +
  geom_histogram(stat = "count") +
  labs(title = "Histogram of Semester Phase Counts",
       x = "Semester Phase",
       y = "Count")
```
Might think of oversampling the minority class or undersampling the majority class 

```{r}
data_trn |> 
  select(year, semester_id, semester, academic_year, adhd_clinical) |>
  names() |>
  map(\(name) plot_bar(df = data_trn, x = name)) |>
  plot_grid(plotlist = _, ncol = 2)
```

### Bivariate Reltionship between the categorical 
```{r}
data_trn |> 
  plot_grouped_barplot_percent(x = "adhd_clinical", y = "semester_phase")
```

```{r}
data_trn |> 
  plot_grouped_barplot_percent(x = "semester", y = "semester_phase")
```



```{r}
plot_grouped_barplot_count(df = data_trn, x = "semester_phase", y = "semester")
```

How to deal with this imbalancy? 
might think of 

### Bivariate Realtionship between the numeric and categorical
```{r}
data_trn |> plot_grouped_box_violin("semester_phase", "grit_overall")
```

```{r}
data_trn |> 
  select(openness_score, agreeableness_score, growth_mindset_score, grit_effort, grit_interest, grit_overall, adhd_total) |>
  names() |>
  map(\(name) plot_grouped_box_violin(df = data_trn, x = name, y = "semester_phase")) |>
  plot_grid(plotlist = _, ncol = 3)
```

```{r}
data_trn |> 
  select(grit_overall, matrices_hit_rate, ucmrt_hit_rate, sandia_matrices_hit_rate, neuroticism_score, extraversion_score, conscientiousness_score) |>
  names() |>
  map(\(name) plot_grouped_box_violin(df = data_trn, x = name, y = "semester_phase")) |>
  plot_grid(plotlist = _, ncol = 3)
```


```{r}
data_trn |> 
  select(grit_overall, matrices_hit_rate, neuroticism_score, extraversion_score, conscientiousness_score, openness_score, agreeableness_score, growth_mindset_score, adhd_total) |> 
  cor(use = "pairwise.complete.obs") |> 
  corrplot::corrplot.mixed(tl.pos = "lt")
```


## Fit Decision Trees

```{r}
data_balanced <- data_trn |> 
  group_by(semester_phase) |> 
  sample_n(size = max(table(data_trn$semester_phase)), replace = TRUE) |> 
  ungroup()

```


Instead of the bootstrap, can think of the k-fold being used as validation method 
```{r}
set.seed(20010429)
splits_cv <- data_balanced |> 
  vfold_cv(v = 10, strata = "semester_phase")  # 10-fold cross-validation
```


```{r}
data_balanced <- data_balanced |> 
  mutate(semester_phase = as.factor(semester_phase))

rec <- recipe(semester_phase ~ grit_overall + matrices_hit_rate + neuroticism_score + extraversion_score + conscientiousness_score + openness_score + agreeableness_score + adhd_total + growth_mindset_score + semester, data = data_trn) |> 
  step_impute_median(all_numeric_predictors()) |> 
  step_impute_mode(all_nominal_predictors()) 

rec_prep <- rec |>
  prep(data_balanced)

feat_trn <- rec_prep |> 
  bake(NULL)

feat_test <- rec_prep |> 
  bake(data_test)

feat_trn |> skim_some()
```



```{r}
grid_tree <- grid_regular(cost_complexity(), tree_depth(), min_n(), levels = 4)

grid_tree
```

```{r}
fits_tree <- 
    decision_tree(cost_complexity = tune(),
                tree_depth = tune(),
                min_n = tune()) |>
    set_engine("rpart") |>
    set_mode("classification") |> 
    tune_grid(preprocessor = rec, 
              resamples = splits_cv, 
              grid = grid_tree, 
              metrics = metric_set(roc_auc))

```


```{r}
autoplot(fits_tree)
```




```{r}
show_best(fits_tree)
```

```{r}
fit_tree <-   
  decision_tree(cost_complexity = select_best(fits_tree)$cost_complexity,
                tree_depth = select_best(fits_tree)$tree_depth,
                min_n = select_best(fits_tree)$min_n) |>
  set_engine("rpart", model = TRUE) |>
  set_mode("classification") |>  
  fit(semester_phase ~ ., data = feat_trn)
```

```{r}
fit_tree$fit |> rpart.plot::rpart.plot()
# adhd_clinical or adhd_total 
```


```{r}
cm_2 <- tibble(truth = feat_test$semester_phase,
             estimate = predict(fit_tree, feat_test)$.pred_class) |> 
  conf_mat(truth, estimate)


cm_2
```
```{r}
cm_2 %>% summary()
```

```{r}
autoplot(cm_2)
```

*Indicating that the model has very low prediction power* 


## Fit Bootstrap Aggregating Prediction Model 


## Fit Random Forest 
```{r}
set.seed(20010429)
splits_boot <- data_trn |> 
  bootstraps(times = 100, strata = "semester_phase")
```

```{r}
rec_rf_prep <- rec |>
  prep(data_trn)

feat_trn_rf <- rec_rf_prep |> 
  bake(NULL)

feat_test_rf <- rec_rf_prep |>
  bake(data_test)
```


1. Can also report the results;  
2. Force the depth to be a constant; 
For next week: 
1. top priority: what I have done with the decision tree 
 - recipe (variables in the model)
 - samples (train and test, show the the imbalance of the outcome in the sample, how I deal with it)
 - results (the hyperparameters for the model, fit)
 - How I refine the model

### Hyperparameters
```{r}
grid_rf <- expand_grid(trees = c(250, 500, 750, 1000), 
                       mtry = c(1, 5, 10, 20),
                       min_n = c(1, 2, 5, 10))
```


```{r}
fits_rf <-cache_rds(
  expr = {
    rand_forest(trees = tune(),
              mtry = tune(),
              min_n = tune()) |>
    set_engine("ranger",
               respect.unordered.factors = "order",
               importance = "impurity", 
               oob.error = FALSE,
               seed = 20010429) |>
    set_mode("classification") |> 
    tune_grid(preprocessor = rec, 
              resamples = splits_boot, 
              grid = grid_rf, 
              metrics = metric_set(roc_auc))

  },
  rerun = rerun_setting,
  dir = file_path,
  file = "fits_rf")
```

```{r}
autoplot(fits_rf)
```

Telling why different parameter, why this range of the parameter!!


```{r}
show_best(fits_rf)
```


# Explain what roc_auc and estimator 
# Upsample the random forest 


```{r}
fit_rf <-   
  rand_forest(trees = select_best(fits_rf)$trees,
                mtry = select_best(fits_rf)$mtry,
                min_n = select_best(fits_rf)$min_n) |>
  set_engine("ranger", 
             respect.unordered.factors = "order", 
             importance = "impurity", 
             oob.error = FALSE,
             seed = 20010429) |>
  set_mode("classification") |>  
  fit(semester_phase ~ ., data = feat_trn_rf)
```


```{r}
cm_3 <- tibble(truth = feat_test_rf$semester_phase,
             estimate = predict(fit_rf, feat_test_rf)$.pred_class) |> 
  conf_mat(truth, estimate)


cm_3
```

```{r}
cm_3 %>% summary()
```

```{r}
autoplot(cm_3)
```

```{r}
importance_rf <- vip(fit_rf, method = "model", num_features = 10)
```


```{r}
importance_rf
```

Feature Importance

```{r}
rec_full_prep <- rec |> 
  prep(d_try)

feat_full <-  rec_full_prep |> 
  bake(d_try)
```


```{r}
fit_rf_full <-   
  rand_forest(trees = select_best(fits_rf)$trees,
                mtry = select_best(fits_rf)$mtry,
                min_n = select_best(fits_rf)$min_n) |>
  set_engine("ranger", 
             respect.unordered.factors = "order", 
             importance = "impurity", 
             oob.error = FALSE,
             seed = 20010429) |>
  set_mode("classification") |>  
  fit(semester_phase ~ ., data = feat_full)
```


```{r}
x <- feat_full |> select(-semester_phase)

y <- feat_full |> 
  mutate(semester_phase = factor(semester_phase, levels = c("beginning", "middle", "end"))) |> 
  pull(semester_phase)

```



```{r}
predict_wrapper <- function(model, newdata) {
  predict(model, newdata)$.pred_class  # Returns the predicted class
}
```

```{r}
explain_full <- explain_tidymodels(
  fit_rf_full,       # model object
  data = x,          # features without outcome
  y = y,             # outcome vector
  predict_function = predict_wrapper,
  label = "Random Forest Multiclass"  # Optional label for clarity
)

```

```{r}
library(yardstick)


accuracy_wrapper <- function(observed, predicted) {
  # Convert observed and predicted to factors with three levels
  observed <- factor(observed, levels = c("beginning", "middle", "end"))
  predicted <- factor(predicted, levels = c("beginning", "middle", "end"))
  
  # Ensure lengths match
  if (length(observed) != length(predicted)) {
    stop("Length of observed and predicted do not match.")
  }
  
  # Calculate accuracy
  accuracy_vec(observed, predicted)
}


```

```{r}
set.seed(20010429)
imp_permute <- model_parts(explain_full, 
                           type = "raw", 
                           loss_function = accuracy_wrapper,
                           B = 100)

```


```{r}
imp_permute 
```

```{r}
plot(imp_permute)
```

```{r}
full_model <- imp_permute |>  
    filter(variable == "_full_model_")
  
imp_permute |> 
  filter(variable != "_full_model_",
         variable != "_baseline_") |> 
  mutate(variable = fct_reorder(variable, dropout_loss)) |> 
  ggplot(aes(dropout_loss, variable)) +
  geom_vline(data = full_model, aes(xintercept = dropout_loss),
             linewidth = 1.4, lty = 2, alpha = 0.7) +
  geom_boxplot(fill = "#91CBD765", alpha = 0.4) +
  theme(legend.position = "none") +
  labs(x = "accuracy", 
       y = NULL,  fill = NULL,  color = NULL)
```

```{r}
set.seed(20010429)
imp_permute_group <- model_parts(explain_full, 
                               type = "raw", 
                               loss_function = accuracy_wrapper,
                               B = 100,
                               variable_groups = list(personality_test = 
                                                        c("openness_score",
                                                          "neuroticism_score",
                                                          "extraversion_score", 
                                                          "conscientiousness_score",
                                                          "agreeableness_score")))
```

```{r}
imp_permute_group
```

# For the linear model, we can also upsample the data for minority classes
# Figure out the random forest model hyperparameters, the interpretation of results (also upsample it)
# SOW: Have the intro, method, descriptive of data for presentation 
# story? two multivariate models (linear and random forest)
# pca of the independent variables, then knn? 
# regularization and penalizaed models 
# try KNN, 


