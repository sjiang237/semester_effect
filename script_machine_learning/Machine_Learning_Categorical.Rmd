---
title: "Machine_Learning_Days"
author: "Shuheng Jiang"
date: "2024-10-19"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Setup

### Handle conflicts
```{r}
#| message: false
options(conflicts.policy = "depends.ok")
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/fun_ml.R?raw=true")
tidymodels_conflictRules()
# We also will need to resolve a new conflict using the following code
# Alternatively, John demonstrates code you can use when you load this library to prevent conflicts
conflictRules("Matrix", mask.ok = c("expand", "pack", "unpack"))
```

### Load required packages 
```{r}
#| message: false
#| warning: false
library(kableExtra, exclude = "group_rows") # for displaying formatted tables w/ kbl()
library(janitor, include.only = c("clean_names", "tabyl"))
library(cowplot, include.only = "plot_grid")
library(tidyverse) # for general data wrangling
library(tidymodels) # for modeling
library(kknn)
library(discrim, exclude = "smoothness")
library(xfun, include.only = "cache_rds")
library(cowplot, include.only = "plot_grid")
library(readxl)
library(dplyr)
library(lubridate)
library(skimr)
library(tidyselect)
library(tibble)
library(effectsize)
```

Be sure to initiate parallel processing and set up your path
```{r}
cl <- parallel::makePSOCKcluster(parallel::detectCores(logical = FALSE))
doParallel::registerDoParallel(cl)
```

### Source function scripts
```{r}
#| message: false
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/fun_plots.R?raw=true")
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/fun_eda.R?raw=true")
source("C:/Users/16089/OneDrive/Desktop/lab psych610/610710_functions.R")
```


### Specify other global settings
```{r}
#| include: false
theme_set(theme_classic())
options(tibble.width = Inf, dplyr.print_max=Inf)
rerun_setting <- TRUE
```

### file path
```{r}
file_path <- "C:/Users/16089/OneDrive/Desktop/Capstone_Project/semester_effect/cleaned_data_postsurvey"
```

```{r}
d_try <- read_csv(here::here(file_path, "Generic_Post_Survey_Combined.csv"), 
                   col_types = cols())|>
  glimpse()
```
```{r}
d_try$semester_phase <- cut(
  d_try$days_since_semester_start,
  breaks = c(-Inf, 39, 74, Inf),  # Define breakpoints based on days
  labels = c("start", "middle", "end"),
  include.lowest = TRUE
)
```
Linear SVM; show whether the group differ by mixed components, then 

```{r}
d_try <- d_try %>% select(participant_id, semester_phase, everything())
```

```{r}
d_try <- d_try %>% select(-days_since_semester_start)
```


## EDA for Data Cleaning
### Variable Classes and Tidy Names

Note that from our glimpse() above that all variable names are already lower case/snake case, but year should not be treated as double, adhd_clinical from the Numeric EDA below(or the background knowledge I have when I'm computing the composite score), should be treated as factor variable. 

```{r}
d_try <- d_try |>
  mutate(across(where(is.character), factor)) |>
  mutate(adhd_clinical = factor(adhd_clinical, levels = c("0", "1"), labels = c("no", "yes")),
         year = factor(year, levels = 2018:2023)) |>
  glimpse()
```

### Skim Data
```{r}
d_try|> 
  skim_some() 
```
**Variable class notes:**
*All variables are type factor, type numeric, or date and time. Some changes were made so that the classes as they were read in match their characteristics. The year variable represents the year participants answer the survey, so we class it as factor. We also classed adhd_clinical as factor with labels 'no' and 'yes'*


### Missing Data

```{r eval=FALSE}
d_try |> 
  skim_some() |> 
  select(skim_variable, n_missing, complete_rate)
```


```{r}
d_try |> filter(is.na(adhd_clinical)) |> 
  kbl(align = "r") |> 
  kable_styling(bootstrap_options = c("striped", "condensed")) |> 
  scroll_box(height = "500px", width = "100%")
```
*These might be the data you want to remove*


```{r}
d_try |> filter(is.na(growth_mindset_score)) |> 
  kbl(align = "r") |> 
  kable_styling(bootstrap_options = c("striped", "condensed")) |> 
  scroll_box(height = "500px", width = "100%")
```

*For those who have NAs for growth mindset also has NAs for matrices tests scores*

### Handle Missingness 
```{r}
ids_to_remove <- c("SEP_03_0020", "SEP_03_0039", "SEP_03_00240")  # Replace with your actual participant IDs

# Remove the specified participant IDs from the dataset
d_try <- d_try %>%
  filter(!participant_id %in% ids_to_remove)
```
*Need to further discuss how to handle the missing values for matrices tests and mindset tests* 

```{r}
d_try <- d_try |>
  filter(!is.na(ucmrt_hit_rate) & !is.na(matrices_hit_rate) & !is.na(sandia_matrices_hit_rate))
```

```{r eval=FALSE}
d_try |> 
  skim_some() |> 
  select(skim_variable, n_missing, complete_rate)
```

*Still Growth Mindset Score with missing data left*

```{r}
d_try |> filter(is.na(growth_mindset_score)) |> 
  kbl(align = "r") |> 
  kable_styling(bootstrap_options = c("striped", "condensed")) |> 
  scroll_box(height = "500px", width = "100%")
```

*For those who only miss the growth mindset score, we want to impute the score using median*

### Numeric variables
```{r eval=FALSE}
# skim data, looking at numeric min and max values
d_try |>
  skim_some() |> 
  filter(skim_type == "numeric") |>  # Select only numeric variables since min/max only apply to them
  select(skim_variable, numeric.p0, numeric.p100)
```
*Here, no responses seem out of the ordinary*

### Categorical Variables
**Tidy response labels for categorical variables:** 
```{r}
# tidy the response
d_try <- d_try |> 
  mutate(across(where(is.factor), tidy_responses))

d_try |> 
  select(semester_id, year, academic_year,semester, adhd_clinical, adhd_total) |>
  walk(\(column) print(levels(column)))
```

Now that we have completed data cleaning, we will split our data into train and test sets and save out the cleaned files.

### Generate a Train Test Split
We'll set a seed so that the same split can be redone if needed (like if we find more errors during modeling EDA/feature engineering & need to come back to this script). Assign 25% of the data to be our validation set. Stratify this split on the `days_since_semester_start` outcome variable.
```{r}
set.seed(0429)
splits <- d_try |> 
  initial_split(prop = 3/4, strata = "semester_phase", breaks = 4)
```

### Save cleaned files

```{r}
splits |> 
  analysis() |>
  glimpse() |> 
  write_csv(here::here(file_path, "data_2_train.csv"))

splits |> 
  assessment() |>
  glimpse() |> 
  write_csv(here::here(file_path, "data_2_test.csv"))
```
Complete modeling EDA on the cleaned training data so that I'm able to do good feature engineering & model fitting

## EDA for Modeling
```{r}
clean_class <- function(df){
  
  df |>
  mutate(across(where(is.character), factor)) |>
  mutate(adhd_clinical = fct_relevel(adhd_clinical, c("yes", "no")),
         year = factor(year, levels = 2018:2023),
         semester_phase = fct_relevel(semester_phase, c("start", "middle", "end"))) 
}
```

```{r}
data_trn <- 
  read_csv(here::here(file_path, "data_2_train.csv"), 
           col_types = cols()) |> 
  clean_class() |>
  glimpse()

data_test <- 
  read_csv(here::here(file_path, "data_2_test.csv"), 
           col_types = cols()) |> 
  clean_class()
```

```{r}
data_trn |> skim_all()
```

### Univaraite Exploration of Numeric variables
```{r}
data_trn |> 
  select(grit_overall, matrices_hit_rate, ucmrt_hit_rate, sandia_matrices_hit_rate, neuroticism_score, extraversion_score, conscientiousness_score, openness_score, agreeableness_score, adhd_total, growth_mindset_score, grit_effort, grit_interest, grit_overall) |>
  names() |> 
  map(\(name) plot_box_violin(df = d_try, x = name)) |> 
  plot_grid(plotlist = _, ncol = 4)
```

### Bivariate Exploration of Numeric Variables
#### Correaltion Matrix
```{r}
data_trn |>
  select(grit_overall, matrices_hit_rate, ucmrt_hit_rate, sandia_matrices_hit_rate, neuroticism_score, extraversion_score, conscientiousness_score, openness_score, agreeableness_score, growth_mindset_score, grit_effort, grit_interest, grit_overall, adhd_total) |> 
  cor(use = "pairwise.complete.obs") |> 
  corrplot::corrplot.mixed(tl.pos = "lt")
```


#### Table for Summary Statistics
```{r}
data_trn |> 
  skim_all() |> 
  filter(skim_type == "numeric")
```

### Univaraite Exploration of Categorical variables
This is a binomial distribution
```{r}
ggplot(data_trn, aes(x = semester_id)) +
  geom_histogram(stat = "count") +
  labs(title = "Histogram of Semester ID Counts",
       x = "Semester ID",
       y = "Count")
```

### Univaraite Exploration of Categorical variables
This is a binomial distribution
```{r}
ggplot(data_trn, aes(x = semester_phase)) +
  geom_histogram(stat = "count") +
  labs(title = "Histogram of Semester Phase Counts",
       x = "Semester Phase",
       y = "Count")
```


```{r}
data_trn |> 
  select(year, semester_id, semester, academic_year, adhd_clinical) |>
  names() |>
  map(\(name) plot_bar(df = data_trn, x = name)) |>
  plot_grid(plotlist = _, ncol = 2)
```

### Bivariate Reltionship between the categorical 
```{r}
data_trn |> 
  plot_grouped_barplot_percent(x = "adhd_clinical", y = "semester_phase")
```

```{r}
data_trn |> 
  plot_grouped_barplot_percent(x = "semester", y = "semester_phase")
```



```{r}
plot_grouped_barplot_count(df = data_trn, x = "semester_phase", y = "semester")
```

How to deal with this imbalancy? 

### Bivariate Realtionship between the numeric and categorical
```{r}
data_trn |> plot_grouped_box_violin("semester_phase", "grit_overall")
```

```{r}
data_trn |> 
  select(openness_score, agreeableness_score, growth_mindset_score, grit_effort, grit_interest, grit_overall, adhd_total) |>
  names() |>
  map(\(name) plot_grouped_box_violin(df = data_trn, x = name, y = "semester_phase")) |>
  plot_grid(plotlist = _, ncol = 3)
```

```{r}
data_trn |> 
  select(grit_overall, matrices_hit_rate, ucmrt_hit_rate, sandia_matrices_hit_rate, neuroticism_score, extraversion_score, conscientiousness_score) |>
  names() |>
  map(\(name) plot_grouped_box_violin(df = data_trn, x = name, y = "semester_phase")) |>
  plot_grid(plotlist = _, ncol = 3)
```
## Fit Decision Trees
Instead of the bootstrap, can think of the k-fold being used as validation method 
```{r}
set.seed(20010429)
splits_boot <- data_trn |> 
  bootstraps(times = 100, strata = "semester_phase")
```

```{r}
rec <- recipe(semester_phase ~ ., data = data_trn) |> 
  step_rm(semester_id, academic_year, participant_id, participant_id_dataset, date,   dataset_of_origin, source, first_day_of_the_semester, year) |>
  step_impute_median(all_numeric_predictors()) |> 
  step_impute_mode(all_nominal_predictors())

rec_prep <- rec |>
  prep(data_trn)

feat_trn <- rec_prep |> 
  bake(NULL)

feat_test <- rec_prep |> 
  bake(data_test)
```

```{r}
grid_tree <- grid_regular(cost_complexity(), tree_depth(), min_n(), levels = 4)

grid_tree
```

```{r}
fits_tree <- cache_rds(
  expr = {
    decision_tree(cost_complexity = tune(),
                tree_depth = tune(),
                min_n = tune()) |>
    set_engine("rpart") |>
    set_mode("classification") |> 
    tune_grid(preprocessor = rec, 
              resamples = splits_boot, 
              grid = grid_tree, 
              metrics = metric_set(roc_auc))

  },
  rerun = rerun_setting,
  dir = file_path,
  file = "fits_tree")
```


```{r}
autoplot(fits_tree)
```




```{r}
show_best(fits_tree)
```

```{r}
fit_tree <-   
  decision_tree(cost_complexity = select_best(fits_tree)$cost_complexity,
                tree_depth = select_best(fits_tree)$tree_depth,
                min_n = select_best(fits_tree)$min_n) |>
  set_engine("rpart", model = TRUE) |>
  set_mode("classification") |>  
  fit(semester_phase ~ ., data = feat_trn)
```

```{r}
fit_tree$fit |> rpart.plot::rpart.plot()
```


```{r}
cm_2 <- tibble(truth = feat_test$semester_phase,
             estimate = predict(fit_tree, feat_test)$.pred_class) |> 
  conf_mat(truth, estimate)


cm_2
```
```{r}
cm_2 %>% summary()
```

*Indicating that the model has very low prediction power* 

Run a Shap Value Instead Probably....



## Fit Bootstrap Aggregating Prediction Model 


## Fit Random Forest 
```{r}
set.seed(20010429)
splits_boot <- data_trn |> 
  bootstraps(times = 100, strata = "semester_phase")
```

```{r}

```

### Hyperparameters
```{r}
grid_rf <- expand_grid(trees = c(250, 500, 750, 1000), 
                       mtry = c(1, 5, 10, 20),
                       min_n = c(1, 2, 5, 10))
```

```{r}
rec_rf <- recipe(semester_phase ~ ., data = data_trn) |> 
  step_rm(semester_id, academic_year, participant_id, participant_id_dataset, date,   dataset_of_origin, source) |>
  step_impute_median(all_numeric_predictors()) |> 
  step_impute_mode(all_nominal_predictors())

rec_rf_prep <- rec_rf |>
  prep(data_trn)

feat_trn_rf <- rec_rf_prep |> 
  bake(NULL)

feat_test_rf <- rec_rf_prep |> 
  bake(data_test)
```


```{r}
fits_rf <-cache_rds(
  expr = {
    rand_forest(trees = tune(),
              mtry = tune(),
              min_n = tune()) |>
    set_engine("ranger",
               respect.unordered.factors = "order",
               oob.error = FALSE,
               seed = 20010429) |>
    set_mode("classification") |> 
    tune_grid(preprocessor = rec_rf, 
              resamples = splits_boot, 
              grid = grid_rf, 
              metrics = metric_set(roc_auc))

  },
  rerun = rerun_setting,
  dir = file_path,
  file = "fits_rf")
```

